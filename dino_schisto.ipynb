{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TroTusk/dino_schisto/blob/main/dino_schisto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p2nhq8QVz4a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget \"https://zenodo.org/records/10908163/files/DinoBloom-S.pth?download=1\" -O dinobloom-s.pth\n",
        "#!wget \"https://zenodo.org/records/10908163/files/DinoBloom-L.pth?download=1\" -O dinobloom-l.pth\n",
        "!wget \"https://zenodo.org/records/10908163/files/DinoBloom-B.pth?download=1\" -O dinobloom-b.pth\n",
        "!git clone https://github.com/zxaoyou/segmentation_WBC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXfO23ajeg-u",
        "outputId": "c47e878f-fe19-47fa-e81a-adbd0da68106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-15 09:53:55--  https://zenodo.org/records/10908163/files/DinoBloom-B.pth?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.103.159, 188.185.79.172, 188.184.98.238, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 528094030 (504M) [application/octet-stream]\n",
            "Saving to: ‘dinobloom-b.pth’\n",
            "\n",
            "dinobloom-b.pth     100%[===================>] 503.63M  22.6MB/s    in 25s     \n",
            "\n",
            "2024-09-15 09:54:21 (20.3 MB/s) - ‘dinobloom-b.pth’ saved [528094030/528094030]\n",
            "\n",
            "Cloning into 'segmentation_WBC'...\n",
            "remote: Enumerating objects: 898, done.\u001b[K\n",
            "remote: Total 898 (delta 0), reused 0 (delta 0), pack-reused 898 (from 1)\u001b[K\n",
            "Receiving objects: 100% (898/898), 26.08 MiB | 32.25 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "# PCA for feature inferred\n",
        "from sklearn.decomposition import PCA\n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "import random\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "HpNxo6Ibej2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "#import rearrange from einops\n",
        "from einops import rearrange\n",
        "\n",
        "embed_sizes={\"dinov2_vits14\": 384,\n",
        "        \"dinov2_vitb14\": 768,\n",
        "        \"dinov2_vitl14\": 1024,\n",
        "        \"dinov2_vitg14\": 1536}\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "patch_num_h=16\n",
        "patch_num_w=16\n",
        "img_size=224\n",
        "eval_model=\"dinov2_vitb14\"\n",
        "\n",
        "def get_dino_bloom(modelpath=\"/content/dinobloom-b.pth\",modelname=\"dinov2_vitb14\"):\n",
        "    # load the original DINOv2 model with the correct architecture and parameters.\n",
        "    model=torch.hub.load('facebookresearch/dinov2', modelname)\n",
        "    # load finetuned weights\n",
        "    pretrained = torch.load(modelpath, map_location=torch.device('cpu'))\n",
        "    # make correct state dict for loading\n",
        "    new_state_dict = {}\n",
        "    for key, value in pretrained['teacher'].items():\n",
        "        if 'dino_head' in key or \"ibot_head\" in key:\n",
        "            pass\n",
        "        else:\n",
        "            new_key = key.replace('backbone.', '')\n",
        "            new_state_dict[new_key] = value\n",
        "\n",
        "    #corresponds to 224x224 image. patch size=14x14 => 16*16 patches\n",
        "    pos_embed = nn.Parameter(torch.zeros(1, 257, embed_sizes[\"dinov2_vitb14\"]))\n",
        "    model.pos_embed = pos_embed\n",
        "\n",
        "    model.load_state_dict(new_state_dict, strict=True)\n",
        "    return model\n",
        "\n",
        "dinov2_model=get_dino_bloom()"
      ],
      "metadata": {
        "id": "seWI3XX7d-Ki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba9afcb-3020-45ca-9974-8d42feb641b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 254MB/s]\n",
            "<ipython-input-3-b52e27bc68dc>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained = torch.load(modelpath, map_location=torch.device('cpu'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PTEd_RHEQ7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ccba70-72c7-46d2-92e7-0175a83ae359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1024, 768])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "#[TASK 2] Provare con diverse configurazioni di parametri\n",
        "'''\n",
        "Modelli disponibili\n",
        "# DINOv2\n",
        "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
        "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
        "dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
        "\n",
        "# DINOv2 with registers\n",
        "dinov2_vits14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
        "dinov2_vitb14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
        "dinov2_vitl14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
        "dinov2_vitg14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')\n",
        "'''\n",
        "# Step 1: Load the DINOv2 model and modify for SIZExSIZE input\n",
        "#dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "\n",
        "\n",
        "\n",
        "dinov2_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "FREEZE_DINO = True\n",
        "SIZE = 448 # 224, 448, 672\n",
        "MODEL_SIZE = 768 #cambiare se cambia il modello s 384  b 768 l 1024 g 1536 eccetera\n",
        "\n",
        "#HINT se da errore out of memory ridurre questo parametro\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "# Step 2: Freeze DINOv2 model parameters\n",
        "if FREEZE_DINO:\n",
        "  for param in dinov2_model.parameters():\n",
        "      param.requires_grad = False\n",
        "x = torch.rand(2, 3, SIZE,SIZE)\n",
        "\n",
        "#print shape of the features\n",
        "print(dinov2_model.forward_features(x)['x_norm_patchtokens'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#segmentation head model\n",
        "class SegmentationHead( nn.Module ):\n",
        "    def __init__( self, in_channels, out_channels,dino ):\n",
        "        super( SegmentationHead, self ).__init__()\n",
        "        self.extrator = dino\n",
        "        self.to_label = nn.Sequential(\n",
        "            nn.Linear( in_channels, in_channels ),\n",
        "            nn.Linear( in_channels, out_channels ),\n",
        "        )\n",
        "        self.cnn_deconder_layer_num = 4\n",
        "        self.inner_channels = 16\n",
        "        self.drop = 0.0\n",
        "        self.cnn_deconder_layer = nn.Sequential(\n",
        "            #nn.BatchNorm2d(self.inner_channels),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Conv2d(self.inner_channels, self.inner_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.GELU()\n",
        "            )\n",
        "        self.cnn_deconder_layers = nn.ModuleList( [self.cnn_deconder_layer] * self.cnn_deconder_layer_num )\n",
        "        self.exp_conv = nn.Conv2d( 1, self.inner_channels, kernel_size=7, stride=1, padding=3 )\n",
        "        self.dec_conv = nn.Conv2d( self.inner_channels, 1, kernel_size=7, stride=1, padding=3 )\n",
        "\n",
        "    def forward( self, x ):\n",
        "        x = self.extrator.forward_features(x)['x_norm_patchtokens']\n",
        "        x = self.to_label( x )\n",
        "        #rearrange\n",
        "        # Assuming that the ViT divides the image into non-overlapping patches of size (patch_size x patch_size)\n",
        "        patch_size = 14  # Replace with the actual patch size used in the ViT\n",
        "\n",
        "        # Calculate the number of patches along each dimension\n",
        "        n_patches = SIZE // patch_size  # Assuming the input image is SIZExSIZE\n",
        "\n",
        "        # Reshape and rearrange the patches to reconstruct the image\n",
        "        x = x.view(x.shape[0], n_patches, n_patches, patch_size, patch_size)\n",
        "        x = x.permute(0, 1, 3, 2, 4)  # Permute dimensions to correctly align patches\n",
        "        x = x.contiguous().view(x.shape[0], SIZE, SIZE)  # Reshape to the original image size\n",
        "        #deconding\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.exp_conv(x)\n",
        "        for layer in self.cnn_deconder_layers:\n",
        "            x = layer(x)\n",
        "        x = self.dec_conv(x)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "seg = SegmentationHead( MODEL_SIZE, 196, dinov2_model ).cuda()\n",
        "#print model summary\n",
        "print(seg)\n",
        "\n",
        "#print shape of the features\n",
        "#print(seg(x).shape)\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        # Flatten the tensors\n",
        "        preds = preds.contiguous().view(-1)\n",
        "        targets = targets.contiguous().view(-1)\n",
        "\n",
        "        # Calculate intersection\n",
        "        intersection = (preds * targets).sum()\n",
        "\n",
        "        # Calculate Dice coefficient\n",
        "        dice_coeff = (2. * intersection + self.smooth) / (preds.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "        # Calculate Dice loss\n",
        "        dice_loss = 1 - dice_coeff\n",
        "\n",
        "        return dice_loss\n",
        "loss = DiceLoss()\n",
        "#y = torch.rand(2, SIZE,SIZE)\n",
        "#print loss\n",
        "#print(loss(seg(x), y))\n",
        "#backward\n",
        "#loss(seg(x), y).backward()\n",
        "\n",
        "#print number of trainable parameters and total number\n",
        "total_params = sum(p.numel() for p in seg.parameters())\n",
        "trainable_params = sum(p.numel() for p in seg.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MXb04EfnEcav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8f749a-d11f-4f5e-b6e3-d0651051f746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SegmentationHead(\n",
            "  (extrator): DinoVisionTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
            "      (norm): Identity()\n",
            "    )\n",
            "    (blocks): ModuleList(\n",
            "      (0-11): 12 x NestedTensorBlock(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): MemEffAttention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): LayerScale()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): LayerScale()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (head): Identity()\n",
            "  )\n",
            "  (to_label): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (1): Linear(in_features=768, out_features=196, bias=True)\n",
            "  )\n",
            "  (cnn_deconder_layer): Sequential(\n",
            "    (0): Dropout(p=0.0, inplace=False)\n",
            "    (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): GELU(approximate='none')\n",
            "  )\n",
            "  (cnn_deconder_layers): ModuleList(\n",
            "    (0-3): 4 x Sequential(\n",
            "      (0): Dropout(p=0.0, inplace=False)\n",
            "      (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (2): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            "  (exp_conv): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "  (dec_conv): Conv2d(16, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            ")\n",
            "Total parameters: 86470917\n",
            "Trainable parameters: 745221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the image asd.jpg and resize to SIZExSIZE\n",
        "#[TASK 3] Provare diverse tecniche di augmentation vedi  https://albumentations.ai/docs/getting_started/mask_augmentation/\n",
        "#Nota bene che per \"augmentare\" le immagini del dataset devi lavorare simultaneamente su maschera di segmentazione e immagine per\n",
        "#evitare errori trovi sotto le librerie commentate per iniziare a usare tale libreria\n",
        "#import albumentations as A\n",
        "#from albumentations.pytorch import ToTensorV2\n",
        "def process(img):\n",
        "    img = Image.open(img).convert('RGB')\n",
        "    # Define the transformations\n",
        "    transform_pipeline = transforms.Compose([\n",
        "        transforms.Resize((SIZE, SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Apply transformations\n",
        "    img = transform_pipeline(img)\n",
        "\n",
        "\n",
        "\n",
        "    return img\n",
        "\n"
      ],
      "metadata": {
        "id": "fI7lMPToI4V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fiGDVgmzJCJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f74407-2406-4abf-97d5-c227dd2e226a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import dataset and dataloader from torch\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "import os\n",
        "\n",
        "#create the dataset that output both image and mask\n",
        "\n",
        "class Custom_Dataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, preproceesor):\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.preproceesor = preproceesor\n",
        "\n",
        "        self.images = os.listdir(img_dir)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '_mask.png'))\n",
        "\n",
        "\n",
        "        mask = Image.open(mask_path).convert('L') # Assuming masks are single-channel grayscale\n",
        "\n",
        "        mask = transforms.Resize((SIZE, SIZE))(mask)\n",
        "        mask = transforms.ToTensor()(mask)\n",
        "        image = self.preproceesor(img_path)\n",
        "\n",
        "        return image, mask\n",
        "#split into train val test\n",
        "# TASK [0] adattare il dataset Schistociti al formato richiesto, vedi esempio dataset persone\n",
        "# TASK [1] adattare al dataset Schistociti\n",
        "img_dir = '/content/drive/MyDrive/Colab Notebooks/dataset_schisto_coco/train/images'\n",
        "mask_dir = '/content/drive/MyDrive/Colab Notebooks/dataset_schisto_coco/train/masks'\n",
        "\n",
        "\n",
        "dataset = Custom_Dataset(img_dir=img_dir, mask_dir=mask_dir, preproceesor=process)\n"
      ],
      "metadata": {
        "id": "Xqn7TQGCO3FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoader for each set\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "ckyGMEZqRJCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=seg.cuda()\n",
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "TtNmzsZZRXWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#leggere https://medium.com/@nghihuynh_37300/understanding-evaluation-metrics-in-medical-image-segmentation-d289a373a3f\n",
        "def precision_score_(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    total_pixel_pred = np.sum(pred_mask)\n",
        "    precision = np.mean(intersect/total_pixel_pred)\n",
        "    return round(precision, 3)\n",
        "\n",
        "def recall_score_(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    total_pixel_truth = np.sum(groundtruth_mask)\n",
        "    recall = np.mean(intersect/total_pixel_truth)\n",
        "    return round(recall, 3)\n",
        "\n",
        "def accuracy(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    union = np.sum(pred_mask) + np.sum(groundtruth_mask) - intersect\n",
        "    xor = np.sum(groundtruth_mask==pred_mask)\n",
        "    acc = np.mean(xor/(union + xor - intersect))\n",
        "    return round(acc, 3)\n",
        "\n",
        "def dice_coef(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    total_sum = np.sum(pred_mask) + np.sum(groundtruth_mask)\n",
        "    dice = np.mean(2*intersect/total_sum)\n",
        "    return round(dice, 3) #round up to 3 decimal places\n",
        "\n",
        "def iou(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    union = np.sum(pred_mask) + np.sum(groundtruth_mask) - intersect\n",
        "    iou = np.mean(intersect/union)\n",
        "    return round(iou, 3)\n",
        "\n",
        "def compute_metrics(groundtruth_mask, pred_mask):\n",
        "    pre = precision_score_(groundtruth_mask, pred_mask)\n",
        "    rec = recall_score_(groundtruth_mask, pred_mask)\n",
        "    acc = accuracy(groundtruth_mask, pred_mask)\n",
        "    dice = dice_coef(groundtruth_mask, pred_mask)\n",
        "    iou_ = iou(groundtruth_mask, pred_mask)\n",
        "\n",
        "    #print(f\"Precision: {pre} , Recall: {rec} , Accuracy: {acc} , Dice: {dice} , IoU: {iou_}\")\n",
        "    return pre, rec, acc, dice, iou_\n"
      ],
      "metadata": {
        "id": "gqRwTIedIl9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#DiceLoss()\n",
        "#nn.L1Loss()\n",
        "#nn.BCELoss()\n",
        "#nn.BCEWithLogitsLoss()\n",
        "#criterion = nn.MSELoss()\n",
        "criterion = DiceLoss()\n",
        "model.to(device)\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    a_pre = []\n",
        "    a_rec = []\n",
        "    a_acc = []\n",
        "    a_dice = []\n",
        "    a_iou = []\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "            #compute metrics\n",
        "            pre, rec, acc, dice, iou_ = compute_metrics(masks.cpu().numpy(), outputs.cpu().numpy())\n",
        "            a_pre.append(pre)\n",
        "            a_rec.append(rec)\n",
        "            a_acc.append(acc)\n",
        "            a_dice.append(dice)\n",
        "            a_iou.append(iou_)\n",
        "    print(f\"Precision: {np.mean(a_pre)} , Recall: {np.mean(a_rec)} , Accuracy: {np.mean(a_acc)} , Dice: {np.mean(a_dice)} , IoU: {np.mean(a_iou)}\")\n",
        "\n",
        "    # Save the model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(f'Validation Loss after Epoch {epoch+1}: {val_loss/len(val_loader):.4f}')\n"
      ],
      "metadata": {
        "id": "1o26T2RQz4ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "a_pre = []\n",
        "a_rec = []\n",
        "a_acc = []\n",
        "a_dice = []\n",
        "a_iou = []\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        test_loss += loss.item()\n",
        "        pre, rec, acc, dice, iou_ = compute_metrics(masks.cpu().numpy(), outputs.cpu().numpy())\n",
        "        a_pre.append(pre)\n",
        "        a_rec.append(rec)\n",
        "        a_acc.append(acc)\n",
        "        a_dice.append(dice)\n",
        "        a_iou.append(iou_)\n",
        "print(f\"Precision: {np.mean(a_pre)} , Recall: {np.mean(a_rec)} , Accuracy: {np.mean(a_acc)} , Dice: {np.mean(a_dice)} , IoU: {np.mean(a_iou)}\")\n",
        "print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n"
      ],
      "metadata": {
        "id": "opUOynxoz6nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot last label and prediction in a single plot 2x1\n",
        "\n",
        "def plot_label_and_prediction(label, prediction):\n",
        "    # Convert tensors to numpy arrays for plotting\n",
        "    label = label.squeeze().cpu().numpy()\n",
        "    #if greater than 0.5 is 1 else 0\n",
        "\n",
        "    prediction = prediction.squeeze().cpu().detach().numpy()\n",
        "    prediction[prediction >= 0.5] = 1\n",
        "    prediction[prediction < 0.5] = 0\n",
        "\n",
        "    # Create a 2x1 plot\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
        "\n",
        "    # Plot the label (Ground Truth)\n",
        "    axes[0].imshow(label, cmap='gray')\n",
        "    axes[0].set_title('Ground Truth (Label)')\n",
        "    axes[0].axis('off')  # Turn off axis\n",
        "\n",
        "    # Plot the prediction\n",
        "    axes[1].imshow(prediction, cmap='gray')\n",
        "    axes[1].set_title('Prediction')\n",
        "    axes[1].axis('off')  # Turn off axis\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(masks)):\n",
        "  plot_label_and_prediction(masks[i], outputs[i])\n"
      ],
      "metadata": {
        "id": "NY5TxIdm1Wb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}